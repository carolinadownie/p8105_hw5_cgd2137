---
title: "p8105_hw5_cgd2137"
author: "Carolina Downie"
date: "11/1/2017"
output: html_document
---

```{r loading packages}

library(ggplot2)
library(tidyverse)
library(rvest)
library(httr)
library(stringr)
library(janitor)
library(haven)
library(stringr)
library(forcats)
library(tidytext)
library(viridis)

knitr::opts_chunk$set(
  fig.width = 11,
  fig.asp = .6,
  out.width = "90%")
```

#Problem 1: 
Using the State of New York API, read the complete dataset using functions in httr. By default, the API will return only the first 1000 entries, so using the GET option query = list($limit= 2000) in your request will be useful.

```{r loading and cleaning New York subway data}
nyc_subway <- GET("https://data.ny.gov/resource/hvwh-qtfg.json", query = list("$limit" = 2000)) %>% 
  content("text") %>%
  jsonlite::fromJSON() %>%
  select(station_name, north_south_street, east_west_street, entrance_latitude, entrance_longitude, corner)
```

Make a plot showing the number of entrances for each subway station. Restrict your plot to stations that have more than 10 entrances, and order stations according to the number of entrances.

```{r number of entrances for each subway station}
#This works
nyc_subway %>% group_by(station_name) %>%
  mutate(num_entrances = n()) %>%
  filter(num_entrances > 10) %>%
  select(station_name, num_entrances) %>%
  unique(.) %>%
  ggplot(aes(x = forcats::fct_reorder(station_name, num_entrances), y = num_entrances, group = station_name)) + 
    geom_bar(stat = "identity") + labs(
    title = "Number of entrances for each subway station",
    x = "Subway station",
    y = "Number of entrances") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
  
  ##This doesn't work--This is adding up the num_entrances for each station name (34th street 41--41*41 is ~1600, which is what is being plotted on the graph)

nyc_subway %>% group_by(station_name) %>%
  mutate(num_entrances = n()) %>%
  filter(num_entrances > 10) %>%
  ggplot(aes(x = forcats::fct_reorder(station_name, num_entrances), y = num_entrances, group = station_name)) + 
    geom_bar(stat = "identity") + labs(
    title = "Number of entrances for each subway station",
    x = "Subway station",
    y = "Number of entrances") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```

Overall (not only in stations that have more than 10 entrances), how many subway station names contain the abbreviation “St”? How many end with “St”?

```{r subway stations containing St}

nyc_subway_contains_st<-nyc_subway %>% filter(str_detect(station_name, "St")) 

```

There are `r length(unique(nyc_subway_contains_st$station_name))` subway stations containing "St" in their name. 

```{r subway statins ending in St}
nyc_subway_ends_St<-nyc_subway_contains_st %>% 
  pull(station_name) %>% 
  str_locate(., "St") %>% 
  data.frame() %>% 
  cbind(., nyc_subway_contains_st) %>%
  filter(end == str_length(station_name))

```

There are `r length(unique(nyc_subway_ends_St$station_name))` subway stations with names ending in "St". 

#Problem 2:
I’m curious about how many people watched each episode of “Game of Thrones” over the past 7 seasons. Find these data online and import them into R using functions in rvest. Taking the time to find data that’s pretty close to the format you want is worth a bit of effort; wikipedia is a good place to start.



```{r loading wikipedia chart}
url<- "https://en.wikipedia.org/wiki/Game_of_Thrones#cite_note-S3ratings-277"

ratings_xml <- (read_html(url) %>% html_nodes(css = "table"))[[4]] %>% html_table(header = TRUE) %>% data.frame()

```

After you’ve found and read the data, make sure they’re tidy. In your final dataset, include variables for season, episode, and viewers; also create a unique episode ID of the form SX_EYY where X and Y are season or episode numbers.

```{r tidying GoT chart}

GoT_ratings <- ratings_xml %>% select(-c(Average, Season)) %>% 
  rename(Season = Season.1) %>%
  gather(key = Episode, value = Num_viewers, Ep..1: Ep..10) %>%
  separate(Episode, c("remove", "Episode"), "\\..") %>% 
  select(-remove) %>%
  filter(Num_viewers != "N/A") %>%
  mutate(episode_id = paste("S",substr(Season, 1,1), "_","E",substr(Episode, 1, 2)), Num_viewers = as.numeric(Num_viewers)) 

```

Make a plot that shows the number of viewers for each episode of each season.

```{r number of viewers for each episode of each season}
GoT_ratings %>% 
  mutate(episode_id = fct_reorder(episode_id, Num_viewers)) %>%
  ggplot(aes(x = episode_id, y = Num_viewers)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Number of viewers for each Game of Thrones episode",
    x = "Episode_id",
    y = "Number of viewers (millions)") + 
  theme(legend.position = "bottom") + theme(axis.text.x = element_text(angle = 90, hjust = 1))

#Is this the best way to order the graph??
```


Make a boxplot of the number of viewers for each episode of each season.
```{r boxplot of number of viewers}
GoT_ratings %>%
  mutate(Episode = fct_reorder(Episode, Num_viewers)) %>%
  ggplot(aes(x = Episode, y = Num_viewers)) +
  geom_boxplot() +
  labs(
    title = "Distribution of number of viewers for each Game of Thrones episode across seasons",
    x = "Episode",
    y = "Number of viewers (millions)") + 
  theme(legend.position = "bottom") + theme(axis.text.x = element_text(angle = 90, hjust = 1))



```


Fit a linear model that treats number of viewers in each episode as a response and season as a categorical predictor; make season 4 the reference season. Present and discuss the results of your modeling.

```{r linear model}
GoT_ratings %>% 
  mutate(Season = as.factor(Season), Season = fct_relevel(Season, "4")) %>%
  lm(Num_viewers ~ Season, data = .) %>%
  broom::tidy() %>% 
  select(-std.error, -statistic) %>% 
  knitr::kable(digits = 3)
```


This linear model suggests that for seasons 1, 2, and 3, season had a negative relationship with number of viewers, whereas for seasons 5, 6, and 7, season had a positive relationship with number of viewers (when season 4 was used as the reference group). Season 7 has the largest estimated intercept. 


#Problem 3:
```{r scraping Amazon Napolean Dynamite reviews}

##Why doesn't this consistently scrape 1000 reviews? 
read_page_reviews <- function(url) {
  
  h <- read_html(url)
  
  title <- h %>%
    html_nodes("#cm_cr-review_list .review-title") %>%
    html_text()
  
  stars <- h %>%
    html_nodes("#cm_cr-review_list .review-rating") %>%
    html_text() %>%
    str_extract("\\d") %>%
    as.numeric()
  
  text = h %>%
    html_nodes(".review-data:nth-child(4)") %>%
    html_text()
  
  data_frame(title, stars, text)
}

url_base <- "https://www.amazon.com/product-reviews/B00005JNBQ/ref=cm_cr_arp_d_viewopt_rvwer?ie=UTF8&reviewerType=avp_only_reviews&sortBy=recent&pageNumber="
urls <- paste0(url_base, 1:100)

dynamite_reviews <- map(urls, ~read_page_reviews(.x)) %>% 
  bind_rows
```

Inspect and describe the resulting dataset. What variables are included? Has the scraping been successful?

The dataset created by this procedure contains 3 variables: title, stars, and text. Title stores the initial text that accompanies the star rating, stars stores information about how many stars the reviewer gave, and text stores the additional comments found below the star rating on the Amazon website. However, the titles are non-unique; for example, multiple reviewers titled their reviews "Five Stars". This will likely cause issues when we try to analyze the dataset, so another variable with the unique review number should be added:

```{r}
dynamite_reviews <- dynamite_reviews %>% mutate(review_num = c(1:1000)) %>% select(review_num, title, stars, text)

```


Create a tidy text dataset from the above using the text in the reviews. Use words as the token and remove stop words.

```{r creating tidy text dataset}
ratings_tidy_text<- dynamite_reviews %>% unnest_tokens(word, text)

data(stop_words)

ratings_tidy_text <- anti_join(ratings_tidy_text, stop_words)

```


What words are most frequently used in five-star reviews? In 1-star reviews?

Make a plot that shows the (approximate) log odds ratio for word appearance comparing 1-star reviews to 5-star reviews; include the 10 words with the most extreme log ORs in both directions.

```{r plotting word frequency in 5-star vs 1-star reviews}
#Calculating log odds ratio
ratings_word_ratios <- ratings_tidy_text %>% 
  mutate(stars = as.character(stars)) %>%
  filter(stars %in% c("1", "5")) %>% 
  count(word, stars) %>%
  group_by(word) %>% 
  filter(sum(n) >= 5) %>%
  ungroup() %>%
  spread(stars, n, fill = 0) %>%
  rename(One = "1", Five = "5") %>%
  mutate(
   One_odds = (One + 1) / (sum(One) + 1),
    Five_odds = (Five + 1) / (sum(Five) + 1),
    log_OR = log(One_odds / Five_odds)
  ) %>%
  arrange(desc(log_OR)) 

#Plotting the 10 words with the most extreme log odds 
ratings_word_ratios %>%
  mutate(pos_log_OR = ifelse(log_OR > 0, "One > Five", "Five > One")) %>% 
  group_by(pos_log_OR) %>%
  top_n(10, abs(log_OR)) %>%
  ungroup() %>%
  mutate(word = fct_reorder(word, log_OR)) %>%
  ggplot(aes(word, log_OR, fill = pos_log_OR)) +
  geom_col() +
  coord_flip() +
  ylab("log odds ratio (One/Five)") +
  scale_fill_discrete(name = "") +
  labs(
    title = "Plot of 10 words with more extreme log odds comparing 1-star to 5-star reviews"
  )

```


Conduct a sentiment analysis of the review texts; make a plot of your results and include the star rating in your graphic. What is the most positive review? The most negative review?

```{r sentiment analysis}
bing_sentiments <- get_sentiments("bing")

#Need to add review number because not all titles are unique. 
ratings_sentiments <- ratings_tidy_text %>% 
  inner_join(., bing_sentiments) %>% 
  count(review_num, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative) %>% 
  select(review_num, sentiment)

ratings_sentiments <- right_join(dynamite_reviews, ratings_sentiments, by = "review_num")

ratings_sentiments %>% 
  mutate(review_num = factor(review_num),
    review_num = fct_reorder(review_num, sentiment), 
    stars = as.factor(stars)) %>% 
  ggplot(aes(x = review_num, 
             y = sentiment, fill = stars, color = stars)) + 
  geom_bar(stat = "identity") + 
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  scale_fill_viridis(discrete = TRUE) + 
  scale_color_viridis(discrete = TRUE) +
  labs(
    title = "Sentiment analysis of the review texts, by number of stars"
  )

```

```{r most positive and negative reviews}
most_positive_review <- ratings_sentiments %>% filter(sentiment == max(sentiment))

most_neg_review <- ratings_sentiments %>% filter(sentiment == min(sentiment))

```


The most positive review was review number `r most_positive_review$review_num`, with a sentiment score of `r most_positive_review$sentiment`. 

The most negative review was review number `r most_neg_review$review_num`, with a sentiment score of `r most_neg_review$sentiment`. 
